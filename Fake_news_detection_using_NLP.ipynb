{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQMvo067hONR5SFLPkSQYp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AB0823/Fake-News-Detection-Using-Natural-Language-Process/blob/main/Fake_news_detection_using_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4"
      ],
      "metadata": {
        "id": "5GAkNM1ny711"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEO3p1DivADc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import warnings\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "znSDaER8vK8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "7U6fa3UbvNac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "metadata": {
        "id": "od1QpLmgvPAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = pd.read_csv(\"/content/Fake.csv\")\n",
        "t = pd.read_csv(\"/content/True.csv\")"
      ],
      "metadata": {
        "id": "hfKU6t9kvO8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f[\"category\"] = 1\n",
        "t[\"category\"] = 0"
      ],
      "metadata": {
        "id": "12FjTQFjvO6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([f, t]).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "aSVfUHHKvO2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "graph = sns.countplot(x=\"category\", data=df)\n",
        "plt.title(\"Count of Fake and True News\")\n",
        "for p in graph.patches:\n",
        "    height = p.get_height()\n",
        "    graph.text(p.get_x() + p.get_width() / 2., height + 0.2, height, ha=\"center\", fontsize=12)"
      ],
      "metadata": {
        "id": "tQXDG72DvOzz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "graph = sns.countplot(x=\"subject\", data=df)\n",
        "plt.title(\"Count of Subjects\")\n",
        "for p in graph.patches:\n",
        "    height = p.get_height()\n",
        "    graph.text(p.get_x() + p.get_width() / 2., height + 0.2, height, ha=\"center\", fontsize=12)"
      ],
      "metadata": {
        "id": "zG2aQzxCvOxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = df[\"title\"] + df[\"text\"]\n",
        "df = df[[\"text\", \"category\"]]\n",
        "blanks = [index for index, text in df[\"text\"].items() if isinstance(text, str) and text.isspace()]\n",
        "print(f\"Number of blank (whitespace-only) texts: {len(blanks)}\")"
      ],
      "metadata": {
        "id": "84Im8TNuvOuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "lemma = WordNetLemmatizer()\n",
        "list1 = nlp.Defaults.stop_words\n",
        "list2 = stopwords.words('english')\n",
        "Stopwords = set(set(list1) | set(list2))"
      ],
      "metadata": {
        "id": "XwgZj9bfvOrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"[-()\\\"#!@$%^&*{}?.,:]\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "    return \" \".join(lemma.lemmatize(word) for word in text.split() if word not in Stopwords)"
      ],
      "metadata": {
        "id": "k6AQzmKXvOou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"text\"] = df[\"text\"].apply(clean_text)"
      ],
      "metadata": {
        "id": "AUwFoCNAvOmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df[\"text\"]\n",
        "y = df[\"category\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "VGciXOMkvOjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": X_train, \"label\": y_train}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({\"text\": X_test, \"label\": y_test}))"
      ],
      "metadata": {
        "id": "rtOEcQMfvOgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/bert-base-uncased'\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2, local_files_only=True)"
      ],
      "metadata": {
        "id": "3mhslj8kvOdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "train_dataset = train_dataset.map(tokenize, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize, batched=True)"
      ],
      "metadata": {
        "id": "neTHXWr8vOa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "metadata": {
        "id": "-_eZiP3GvOYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "iBA85KW-vOVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=2,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    eval_steps=None,\n",
        "    fp16=True,  # This should be fine on T4\n",
        "    dataloader_num_workers=4,\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "Xrza5puKvOSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "'''\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=small_train_dataset,\n",
        "    eval_dataset=small_test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")'''"
      ],
      "metadata": {
        "id": "oZY9NG59vOQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "PrlWgehFvw1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "\n",
        "print(f\"Evaluation results: {eval_results}\")"
      ],
      "metadata": {
        "id": "acgu-8pUvwx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = np.argmax(predictions.predictions, axis=1)"
      ],
      "metadata": {
        "id": "nq4kGJ12vwv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BJIJUQduv3pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KcKkNOzwv3mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "11fx6vwUv3ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3AkvO5Qa53I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}